# ===================== üìö 1. Import Library =====================
import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.pipeline import make_pipeline
from scipy import stats
from sklearn.inspection import PartialDependenceDisplay

# ===================== üîç 2. Load and Explore Data =====================
DATA_PATH = os.path.join("..", "..", "AvailableData", "GHG_cleaned_v1.csv")
df = pd.read_csv(DATA_PATH)
print(df.columns)

# ===================== üßπ 3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ =====================
df = df[['GHG', 'CH4', 'CO2', 'F-Gas', 'N2O']]

# ===================== üî¥ 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values =====================
missing_values = df.isnull().sum()
print(f"Missing values:\n{missing_values}")

# ===================== üéØ 5. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Features ‡πÅ‡∏•‡∏∞ Target =====================
X = df[['CH4', 'CO2', 'F-Gas', 'N2O']]  # Feature
y = df['GHG']  # Target

# ===================== üìä 6. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train ‡πÅ‡∏•‡∏∞ Test =====================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ===================== ‚öñÔ∏è 7. Scaling ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• =====================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ===================== üßë‚Äçüíª 8. ‡πÉ‡∏ä‡πâ Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πÄ‡∏Å‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• =====================
pipeline = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=42))
pipeline.fit(X_train, y_train)

# ===================== üß™ 9. Cross-validation ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• =====================
cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')
print(f"Cross-Validation MSE Scores: {cv_scores}")
print(f"Mean Cross-Validation MSE: {cv_scores.mean()}")

# ===================== üìà 10. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• =====================
y_pred_train = pipeline.predict(X_train)
y_pred_test = pipeline.predict(X_test)

# ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ MSE, MAE, ‡πÅ‡∏•‡∏∞ R-squared
mse_train = mean_squared_error(y_train, y_pred_train)
mae_train = mean_absolute_error(y_train, y_pred_train)
r2_train = r2_score(y_train, y_pred_train)

mse_test = mean_squared_error(y_test, y_pred_test)
mae_test = mean_absolute_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
print("Training Metrics:")
print(f"MSE: {mse_train:.2f}")
print(f"MAE: {mae_train:.2f}")
print(f"R-squared: {r2_train:.2%}")

print("\nTesting Metrics:")
print(f"MSE: {mse_test:.2f}")
print(f"MAE: {mae_test:.2f}")
print(f"R-squared: {r2_test:.2%}")

# ===================== üìä 11. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Feature Importance =====================
rf = pipeline.named_steps['randomforestregressor']  # Extract the Random Forest model from the pipeline
feature_importance = rf.feature_importances_
feature_names = X.columns

# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á ‡πÜ
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance, y=feature_names, hue=feature_names, palette='viridis')
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Random Forest Feature Importance")
plt.show()

# ===================== üìâ 12. ‡πÅ‡∏™‡∏î‡∏á Partial Dependence Plots =====================
features = [0, 1, 2, 3]
feature_names = X_train.columns  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ X_train ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå

fig = plt.figure(figsize=(10, 8)) # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î Figure
axes = fig.subplots(2, 2) # ‡∏™‡∏£‡πâ‡∏≤‡∏á Layout ‡πÅ‡∏ö‡∏ö 2x2 Subplots

for i, ax in enumerate(axes.flatten()):
    if i < len(features):
        PartialDependenceDisplay.from_estimator(pipeline, X_train, features=[features[i]],
                                                feature_names=feature_names, ax=ax)
        ax.set_title(f"Partial Dependence: {feature_names[features[i]]}")
    else:
        fig.delaxes(ax) # ‡∏•‡∏ö Subplot ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Features ‡πÑ‡∏°‡πà‡πÄ‡∏ï‡πá‡∏° Layout)

plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # ‡∏õ‡∏£‡∏±‡∏ö Layout ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
fig.suptitle('Partial Dependence Plots', fontsize=16)
plt.show()

# ===================== üîç 13. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Underfitting ‡πÅ‡∏•‡∏∞ Overfitting =====================
train_r2 = r2_train
test_r2 = r2_test

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö R-squared ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
plt.figure(figsize=(8, 6))
plt.bar(['Training', 'Testing'], [train_r2, test_r2], color=['green', 'red'])
plt.ylim(0, 1)
plt.title('Comparison of R-squared: Training vs Testing')
plt.ylabel('R-squared')
plt.show()

# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
print("\nModel Evaluation:")
if r2_train > 0.95 and r2_test < 0.8:
    print("Overfitting: Model performs very well on training data but poorly on test data.")
elif r2_train < 0.7 and r2_test < 0.7:
    print("Underfitting: Model performs poorly on both training and test data.")
else:
    print("Model performance is reasonable.")

# ===================== üìä 14. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Residuals =====================
residuals_train = y_train - y_pred_train
residuals_test = y_test - y_pred_test

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü Residuals
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.scatterplot(x=y_pred_train, y=residuals_train)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Predicted Values (Train)")
plt.ylabel("Residuals (Train)")
plt.title("Residual Plot (Train)")

plt.subplot(1, 2, 2)
sns.scatterplot(x=y_pred_test, y=residuals_test)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Predicted Values (Test)")
plt.ylabel("Residuals (Test)")
plt.title("Residual Plot (Test)")

plt.tight_layout()
plt.show()

# ===================== üß™ 15. ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Normality ‡∏Ç‡∏≠‡∏á Residuals =====================
stat_train, p_value_train = stats.shapiro(residuals_train)
stat_test, p_value_test = stats.shapiro(residuals_test)

print(f"Shapiro-Wilk Test for Train Residuals: Stat={stat_train}, p-value={p_value_train}")
print(f"Shapiro-Wilk Test for Test Residuals: Stat={stat_test}, p-value={p_value_test}")

# ===================== üìä 16. Histogram of Residuals =====================
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(residuals_train, kde=True)
plt.xlabel("Residuals (Train)")
plt.title("Histogram of Residuals (Train)")

plt.subplot(1, 2, 2)
sns.histplot(residuals_test, kde=True)
plt.xlabel("Residuals (Test)")
plt.title("Histogram of Residuals (Test)")

plt.tight_layout()
plt.show()

# ===================== üîÆ 17. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á GHG ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï =====================
y_pred_future = pipeline.predict(X_test)

# ===================== üìä 18. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á =====================
results_df = pd.DataFrame({
    'Actual GHG': y_test,
    'Predicted GHG': y_pred_future
})

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
print(results_df.head())

# ===================== üìà 19. Plot ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå =====================
plt.figure(figsize=(12, 6))
plt.plot(results_df.index, results_df['Actual GHG'], label="Actual GHG", color='b', linewidth=2)
plt.plot(results_df.index, results_df['Predicted GHG'], label="Predicted GHG", color='r', linestyle='--', linewidth=2)

plt.xlabel('Index', fontsize=12)
plt.ylabel('GHG', fontsize=12)
plt.title('Actual vs Predicted GHG', fontsize=14)

plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

# ===================== üíæ 20. Save Model =====================
model_dir = "models"
os.makedirs(model_dir, exist_ok=True)
joblib.dump(pipeline, os.path.join(model_dir, "random_forest_model.pkl"))
joblib.dump(scaler, os.path.join(model_dir, "Random.pkl"))
print("Model saved successfully!")
